{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/projects/home/mmasood1/TG GATE/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import gc\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import os, yaml\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "from sklearn.metrics import average_precision_score, f1_score, mean_absolute_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb.login(key = \"27edf9c66b032c03f72d30e923276b93aa736429\")\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "from molbert.models.smiles import SmilesMolbertModel\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "from molbert.datasets.dataloading import get_dataloaders\n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, StepLR\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "# config_dict\n",
    "model_weights_dir = '/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k_ADME/BERT_pretraining_embeddings_model/encoder_masking_plus_physchem_invitro_invivo_heads/'\n",
    "pretrained_model_path = '/projects/home/mmasood1/TG GATE/MolBERT/molbert/molbert_100epochs/molbert_100epochs/checkpoints/last.ckpt'\n",
    "data_dir = '/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/SMILES_len_th_128/'\n",
    "invitro_pos_weights = \"/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/pos_weights.csv\"\n",
    "invivo_pos_weights = \"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/18_01_2023/Filtered_data_for_BERT/BERT_filtered_preclinical_clinical_pos_weight.csv\"\n",
    "metadata_dir = \"/projects/home/mmasood1/trained_model_predictions/SIDER_PreClinical/BERT_finetune/MF/\"\n",
    "pred_dir = \"/projects/home/mmasood1/trained_model_predictions/SIDER_PreClinical/MolBERT/BERT_pretraining_emb_encoder_masking/predicitons/\"\n",
    "\n",
    "model_dir = os.path.dirname(os.path.dirname(pretrained_model_path))\n",
    "hparams_path = os.path.join(model_dir, 'hparams.yaml')\n",
    "# load config\n",
    "with open(hparams_path) as yaml_file:\n",
    "    config_dict = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dict['project_name'] = \"BERT_pretraining_masking_physchem_invitro\"\n",
    "config_dict['model_name'] = \"BERT_pretraining_emb_encoder_masking\"\n",
    "\n",
    "config_dict['model_weights_dir'] = model_weights_dir\n",
    "config_dict['pretrained_model_path'] = pretrained_model_path\n",
    "config_dict[\"metadata_dir\"] = metadata_dir\n",
    "config_dict[\"pred_dir\"] = pred_dir\n",
    "\n",
    "config_dict['invitro_pos_weights'] = invitro_pos_weights\n",
    "config_dict['invivo_pos_weights'] = invivo_pos_weights\n",
    "config_dict['data_dir'] = data_dir\n",
    "config_dict['invitro_train'] = data_dir + \"train_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "config_dict['invitro_val'] = data_dir + \"test_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "config_dict['invitro_test'] = data_dir + \"test_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "\n",
    "#config_dict['invitro_train'] = data_dir + \"sampled_data/\" + \"invitro_train.pkl\"\n",
    "#config_dict['invitro_val'] = data_dir + \"sampled_data/\" + \"invitro_val.pkl\"\n",
    "#config_dict['invitro_test'] = data_dir + \"sampled_data/\" + \"invitro_val.pkl\"\n",
    "\n",
    "config_dict['max_epochs'] = 1\n",
    "config_dict['unfreeze_epoch'] = 0\n",
    "config_dict[\"l2_lambda\"] = 0.0\n",
    "config_dict['embedding_size'] = 50\n",
    "\n",
    "config_dict['max_seq_length'] = 128\n",
    "config_dict['bert_output_dim'] = 768\n",
    "\n",
    "config_dict['optim'] = 'AdamW'#SGD\n",
    "config_dict['heads_lr'] = 1e-03\n",
    "config_dict[\"BERT_lr\"] = 3e-5\n",
    "config_dict[\"compute_classification\"] = True\n",
    "config_dict[\"save_pred\"] = True\n",
    "\n",
    "config_dict[\"seed\"] = 42\n",
    "config_dict['compute_metric_after_n_epochs'] = 5\n",
    "config_dict['return_trainer'] = True\n",
    "config_dict['EarlyStopping'] = False\n",
    "\n",
    "config_dict[\"accelerator\"] = \"gpu\"\n",
    "config_dict[\"device\"] = torch.device(\"cuda:0\")\n",
    "config_dict[\"precision\"] = 32\n",
    "\n",
    "######## invitro #########################\n",
    "config_dict[\"invitro_batch_size\"] = 64\n",
    "config_dict['invitro_head_hidden_layer'] = 768\n",
    "\n",
    "data = pd.read_pickle(config_dict['invitro_train'])\n",
    "data.drop(['SMILES'], axis = 1, inplace = True)\n",
    "target_names = data.columns.tolist()\n",
    "config_dict[\"output_size\"] = len(target_names)\n",
    "config_dict[\"num_invitro_tasks\"] = len(target_names)\n",
    "config_dict[\"num_of_tasks\"] = len(target_names)\n",
    "config_dict[\"invitro_columns\"] = target_names\n",
    "\n",
    "#### Physchem ##############\n",
    "config_dict[\"num_physchem_properties\"] = 200\n",
    "\n",
    "########### loss ####################\n",
    "config_dict['mode'] = 'classification'\n",
    "config_dict['missing'] = 'nan'\n",
    "config_dict['alpha'] = 1.0\n",
    "config_dict['beta'] = 0.0\n",
    "config_dict['gamma'] = 2.0\n",
    "config_dict['loss_type'] = 'Focal_loss' # 'BCE', 'Focal_loss'\n",
    "\n",
    "############## invivo ###########################\n",
    "config_dict[\"invivo_train\"] = \"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/Data_for_BERT_finetuning/complete_training_set.csv\"\n",
    "config_dict[\"invivo_val\"] = \"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/Data_for_BERT_finetuning/complete_test_set.csv\"\n",
    "config_dict[\"invivo_batch_size\"] = 16\n",
    "config_dict[\"invivo_head_hidden_layer\"] = 128\n",
    "\n",
    "data = pd.read_csv(config_dict['invivo_train'])\n",
    "data.drop(['SMILES','Scafold','fold'], axis = 1, inplace = True)\n",
    "invivo_target_names = data.columns.tolist()\n",
    "config_dict[\"num_invivo_tasks\"] = len(invivo_target_names)\n",
    "config_dict[\"invivo_columns\"] = invivo_target_names\n",
    "###############################################\n",
    "config_dict[\"permute\"] = False\n",
    "\n",
    "config_dict['pretrained_model'] = True\n",
    "config_dict['freeze_level'] = False\n",
    "config_dict[\"gpu\"] = 1\n",
    "config_dict[\"distributed_backend\"] = \"dp\"\n",
    "config_dict[\"pretrained_crash_model\"] = None#\"/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k_ADME/BERT_pretraining_init_MolBERT_masking_physchem_invitro_head/epoch=99-step=0.ckpt\"\n",
    "\n",
    "# make dir to save predictions\n",
    "if not os.path.exists(config_dict[\"pred_dir\"]):\n",
    "    os.makedirs(pred_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "featurizer = SmilesIndexFeaturizer.bert_smiles_index_featurizer(config_dict[\"max_seq_length\"], permute = False)\n",
    "config_dict[\"vocab_size\"] = featurizer.vocab_size\n",
    "\n",
    "invitro_train_dataloader, invitro_val_dataloader = get_dataloaders(\n",
    "                                                                    featurizer = featurizer, \n",
    "                                                                    targets = \"invitro\", \n",
    "                                                                    num_workers = 6,\n",
    "                                                                    config_dict = config_dict)\n",
    "\n",
    "invivo_train_dataloader, invivo_val_dataloader = get_dataloaders(\n",
    "                                                                    featurizer = featurizer, \n",
    "                                                                    targets = \"invivo\", \n",
    "                                                                    num_workers = 6,\n",
    "                                                                    config_dict = config_dict)\n",
    "config_dict[\"num_batches\"] = len(invitro_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "class combine_dataloaders:\n",
    "    \"\"\"\n",
    "    A custom data loader that combines two MolBERT data loaders.\n",
    "    Iterates through the first data loader and cycles through the second data loader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 invitro, \n",
    "                 invivo,\n",
    "                 ):\n",
    "        \n",
    "        self.dataloader1 = invitro\n",
    "        self.dataloader2 = invivo\n",
    "\n",
    "    def __iter__(self):\n",
    "        iter1 = iter(self.dataloader1)\n",
    "        iter2 = cycle(self.dataloader2)\n",
    "        while True:\n",
    "            try:\n",
    "                batch1 = next(iter1)\n",
    "                batch2 = next(iter2)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            yield (batch1,batch2)\n",
    "\n",
    "        logging.info('Epoch finished.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader1)\n",
    "    \n",
    "    # combine invitro and invivo dataloaders\n",
    "train_dataloader = combine_dataloaders(invitro_train_dataloader, invivo_train_dataloader)\n",
    "val_dataloader = combine_dataloaders(invitro_val_dataloader, invivo_val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine invitro and invivo dataloaders\n",
    "train_dataloader = combine_dataloaders(invitro_train_dataloader, invivo_train_dataloader)\n",
    "val_dataloader = combine_dataloaders(invitro_val_dataloader, invivo_val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma, pos_weight):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.w_p = pos_weight\n",
    "\n",
    "\n",
    "    def forward(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Focal Loss function for binary classification.\n",
    "\n",
    "        Arguments:\n",
    "        y_true -- true binary labels (0 or 1), torch.Tensor\n",
    "        y_pred -- predicted probabilities for the positive class, torch.Tensor\n",
    "\n",
    "        Returns:\n",
    "        Focal Loss\n",
    "        \"\"\"\n",
    "        # Compute class weight\n",
    "        p = torch.sigmoid(y_pred)\n",
    "\n",
    "        # Ensure pos_weight is on the same device as y_pred\n",
    "        w_p = self.w_p.to(y_pred.device)\n",
    "\n",
    "        # Compute focal loss for positive and negative examples\n",
    "        focal_loss_pos = - w_p * (1 - p) ** self.gamma * y_true * torch.log(p.clamp(min=1e-8))\n",
    "        focal_loss_pos_neg = - p ** self.gamma * (1 - y_true) * torch.log((1 - p).clamp(min=1e-8))\n",
    "\n",
    "        return focal_loss_pos + focal_loss_pos_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolbertModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args: Namespace):\n",
    "        super().__init__()\n",
    "        #self.automatic_optimization = False\n",
    "        \n",
    "        self.training_step_invitro_labels, self.training_step_invitro_pred = [],[]\n",
    "        self.training_step_invivo_labels, self.training_step_invivo_pred = [],[]\n",
    "        self.training_step_physchem_labels, self.training_step_physchem_pred = [],[]\n",
    "        self.training_step_Masked_token_labels, self.training_step_Masked_token_pred = [],[]\n",
    "\n",
    "        self.val_step_invitro_labels, self.val_step_invitro_pred = [],[]\n",
    "        self.val_step_invivo_labels, self.val_step_invivo_pred = [],[]\n",
    "        self.val_step_physchem_labels, self.val_step_physchem_pred = [],[]\n",
    "        self.val_step_Masked_token_labels, self.val_step_Masked_token_pred = [],[]\n",
    "\n",
    "\n",
    "        self.hparams = args\n",
    "        self.non_weighted_creterian, self.invitro_weighted_creterien, self.invitro_FL = self.get_creterian(args, targets = \"invitro\")\n",
    "        self.non_weighted_creterian, self.invivo_weighted_creterien, self.invivo_FL = self.get_creterian(args, targets = \"invivo\")\n",
    "\n",
    "        # get model, load pretrained weights, and freeze encoder        \n",
    "        model = SmilesMolbertModel(self.hparams)\n",
    "        if self.hparams.pretrained_model:\n",
    "            checkpoint = torch.load(self.hparams.pretrained_model_path, map_location=lambda storage, loc: storage)\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict = False)\n",
    "\n",
    "        self.encoder = model.model.bert\n",
    "        self.Masked_LM_task = model.model.tasks[0]\n",
    "        self.Physchem_task = model.model.tasks[1]\n",
    "        self.invitro_task = model.model.tasks[2]\n",
    "        self.invivo_task = model.model.tasks[3]\n",
    "\n",
    "\n",
    "    def forward(self, batch_inputs):\n",
    "        sequence_output, pooled_output = self.encoder(**batch_inputs)\n",
    "        Masked_token_pred = self.Masked_LM_task(sequence_output, pooled_output)\n",
    "        Physchem_pred = self.Physchem_task(sequence_output, pooled_output)\n",
    "        invitro_pred = self.invitro_task(sequence_output, pooled_output)\n",
    "        invivo_pred = self.invivo_task(sequence_output, pooled_output)\n",
    "\n",
    "        \n",
    "        return Masked_token_pred, Physchem_pred, invitro_pred, invivo_pred\n",
    "    \n",
    "    def get_creterian(self, config, targets):\n",
    "        if targets == \"invitro\":\n",
    "            pos_weights_file = config[\"invitro_pos_weights\"]\n",
    "            selected_tasks = config[\"invitro_columns\"]\n",
    "            num_of_tasks = len(selected_tasks)\n",
    "            if self.hparams.beta > 0:\n",
    "                class_weights_file = config[\"invitro_class_weights\"]\n",
    "\n",
    "\n",
    "        if targets == \"invivo\":\n",
    "            pos_weights_file = config[\"invivo_pos_weights\"]\n",
    "            selected_tasks = config[\"invivo_columns\"]\n",
    "            num_of_tasks = len(selected_tasks)\n",
    "            if self.hparams.beta > 0:\n",
    "                class_weights_file = config[\"invivo_class_weights\"]\n",
    "\n",
    "\n",
    "        # pos weights\n",
    "        if self.hparams.alpha > 0:\n",
    "            pos_weights = pd.read_csv(pos_weights_file)\n",
    "            if self.hparams.num_of_tasks == 1:\n",
    "                pos_weights = pos_weights.set_index(\"Targets\").reindex([selected_tasks]).weights.values\n",
    "            else:\n",
    "                pos_weights = pos_weights.set_index(\"Targets\").reindex(selected_tasks).weights.values\n",
    "            pos_weights = (config[\"alpha\"] * pos_weights) + (1 - config[\"alpha\"])*1\n",
    "            pos_weights = torch.tensor(pos_weights, device = self.device)\n",
    "        else:\n",
    "            pos_weights = torch.tensor([1.0]* num_of_tasks, device = self.device)\n",
    "\n",
    "        alpha_null = torch.isnan(pos_weights).any()\n",
    "        assert not alpha_null, \"There are null values in the pos_weight tensor\"\n",
    "\n",
    "        # class weights\n",
    "        if self.hparams.beta > 0:\n",
    "            if num_of_tasks > 1:\n",
    "                class_weights = pd.read_csv(class_weights_file)\n",
    "                class_weights = class_weights.set_index(\"Targets\").reindex(selected_tasks).weights.values\n",
    "                class_weights = (config[\"beta\"] * class_weights) + (1 - config[\"beta\"])*1\n",
    "                class_weights = torch.tensor(class_weights, device = self.device)\n",
    "            else:\n",
    "                class_weights = torch.tensor([1.0], device = self.device)\n",
    "\n",
    "            beta_null = torch.isnan(class_weights).any()\n",
    "            assert not beta_null, \"There are null values in the class_weight tensor\"\n",
    "\n",
    "            # train_weighted loss, validation no weights\n",
    "            weighted_creterien =  nn.BCEWithLogitsLoss(reduction=\"none\", \n",
    "                                                            pos_weight= pos_weights,\n",
    "                                                            weight= class_weights)\n",
    "        else:\n",
    "            weighted_creterien =  nn.BCEWithLogitsLoss(reduction=\"none\", \n",
    "                                                            pos_weight= pos_weights)\n",
    "        \n",
    "        FL = FocalLoss(gamma=config['gamma'], pos_weight= pos_weights)\n",
    "        non_weighted_creterian =  nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "        return non_weighted_creterian, weighted_creterien, FL\n",
    "     \n",
    "    def l2_regularization(self):\n",
    "        device = torch.device('cuda')\n",
    "        l2_reg = torch.tensor(0., requires_grad=True, device=device)\n",
    "        \n",
    "        # Apply only on weights, exclude bias\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "        return l2_reg\n",
    "    \n",
    "    def add_weight_decay(self, skip_list=()):\n",
    "        decay = []\n",
    "        no_decay = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            if len(param.shape) == 1 or name in skip_list:\n",
    "                no_decay.append(param)\n",
    "            else:\n",
    "                decay.append(param)\n",
    "            \n",
    "        return [\n",
    "            {'params': no_decay, 'weight_decay': 0.},\n",
    "            {'params': decay, 'weight_decay': self.hparams.l2_lambda}]\n",
    "    '''\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_grouped_parameters = self.add_weight_decay(skip_list=())\n",
    "\n",
    "        if self.hparams.optim == 'SGD':\n",
    "            optimizer = torch.optim.SGD(optimizer_grouped_parameters, \n",
    "                                             lr=self.hparams.learning_rate)\n",
    "        if self.hparams.optim == 'Adam':\n",
    "            optimizer = torch.optim.Adam(optimizer_grouped_parameters, \n",
    "                                             lr=self.hparams.learning_rate)\n",
    "        if self.hparams.optim == 'AdamW':    \n",
    "            optimizer = AdamW(optimizer_grouped_parameters, \n",
    "                                lr=self.hparams.learning_rate, \n",
    "                                eps=self.hparams.adam_epsilon)\n",
    "        \n",
    "        scheduler = self._initialise_lr_scheduler(optimizer)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    '''\n",
    "    def configure_optimizers(self):\n",
    "        embedding_params = list(self.encoder.parameters()) + \\\n",
    "                           list(self.Masked_LM_task.parameters())\n",
    "        \n",
    "        Physchem_params = self.Physchem_task.parameters()\n",
    "        invitro_params = self.invitro_task.parameters()\n",
    "        invivo_params = self.invivo_task.parameters()\n",
    "\n",
    "    \n",
    "        embedding_optimizer = torch.optim.AdamW(embedding_params, lr=self.hparams.BERT_lr)\n",
    "        Physchem_optimizer = torch.optim.AdamW(Physchem_params, lr=self.hparams.heads_lr)\n",
    "        invitro_optimizer = torch.optim.AdamW(invitro_params, lr=self.hparams.heads_lr)\n",
    "        invivo_optimizer = torch.optim.AdamW(invivo_params, lr=self.hparams.heads_lr)\n",
    "\n",
    "        embedding_scheduler = self._initialise_lr_scheduler(embedding_optimizer)\n",
    "        Physchem_scheduler = self._initialise_lr_scheduler(Physchem_optimizer)\n",
    "        invitro_scheduler = self._initialise_lr_scheduler(invitro_optimizer)\n",
    "        invivo_scheduler = self._initialise_lr_scheduler(invivo_optimizer)\n",
    "        \n",
    "        return [embedding_optimizer,\n",
    "                Physchem_optimizer,\n",
    "                invitro_optimizer, \n",
    "                invivo_optimizer],[\n",
    "                embedding_scheduler,\n",
    "                Physchem_scheduler,\n",
    "                invitro_scheduler,\n",
    "                invivo_scheduler\n",
    "                ]\n",
    "    \n",
    "    def _initialise_lr_scheduler(self, optimizer):\n",
    "\n",
    "        \n",
    "        num_training_steps = self.hparams.num_batches // self.hparams.accumulate_grad_batches * self.hparams.max_epochs\n",
    "        warmup_steps = int(num_training_steps * self.hparams.warmup_proportion)\n",
    "\n",
    "        if self.hparams.learning_rate_scheduler == 'linear_with_warmup':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_with_hard_restarts_warmup':\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps, num_cycles=1\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_schedule_with_warmup':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant_schedule_with_warmup':\n",
    "            scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_annealing_warm_restarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, warmup_steps)\n",
    "        elif self.hparams.learning_rate_scheduler == 'reduce_on_plateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer)\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant':\n",
    "            scheduler = StepLR(optimizer, 10, gamma=1.0)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'learning_rate_scheduler needs to be one of '\n",
    "                f'linear_with_warmup, cosine_with_hard_restarts_warmup, cosine_schedule_with_warmup, '\n",
    "                f'constant_schedule_with_warmup, cosine_annealing_warm_restarts, reduce_on_plateau, '\n",
    "                f'step_lr. '\n",
    "                f'Given: {self.hparams.learning_rate_scheduler}'\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            f'SCHEDULER: {self.hparams.learning_rate_scheduler} '\n",
    "            f'num_batches={self.hparams.num_batches} '\n",
    "            f'num_training_steps={num_training_steps} '\n",
    "            f'warmup_steps={warmup_steps}'\n",
    "        )\n",
    "\n",
    "        return {'scheduler': scheduler, 'monitor': 'valid_loss', 'interval': 'step', 'frequency': 1}\n",
    "    \n",
    "    def _compute_loss(self, y, y_hat, targets):\n",
    "        if self.hparams.num_of_tasks == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        # compute losses, wiht masking\n",
    "        if self.hparams.missing == 'nan':\n",
    "            nan_mask = torch.isnan(y)\n",
    "            y[nan_mask] = -1\n",
    "            #y = torch.nan_to_num(y, nan = -1), for newer version\n",
    "        \n",
    "        # masks\n",
    "        valid_label_mask = (y != -1).float()\n",
    "        pos_label_mask = (y == 1)\n",
    "        negative_label_mask = (y == 0)\n",
    "\n",
    "        if targets == \"invitro\":\n",
    "            if self.hparams.loss_type == \"BCE\":\n",
    "                weighted_loss = self.invitro_weighted_creterien(y_hat, y) * valid_label_mask\n",
    "            if self.hparams.loss_type == \"Focal_loss\":\n",
    "                weighted_loss = self.invitro_FL(y_hat, y)* valid_label_mask\n",
    "\n",
    "        if targets == \"invivo\":\n",
    "            if self.hparams.loss_type == \"BCE\":\n",
    "                weighted_loss = self.invivo_weighted_creterien(y_hat, y) * valid_label_mask\n",
    "            if self.hparams.loss_type == \"Focal_loss\":\n",
    "                weighted_loss = self.invivo_FL(y_hat, y)* valid_label_mask\n",
    "\n",
    "        Non_weighted_loss = self.non_weighted_creterian(y_hat, y) * valid_label_mask\n",
    "        \n",
    "        # Non_weighted_loss, positive negative loss\n",
    "        pos_loss = Non_weighted_loss * pos_label_mask\n",
    "        neg_loss = Non_weighted_loss * negative_label_mask\n",
    "        pos_loss = pos_loss.sum() / pos_label_mask.sum()\n",
    "        neg_loss = neg_loss.sum() / negative_label_mask.sum()\n",
    "    \n",
    "        # compute mean loss\n",
    "        Non_weighted_loss = Non_weighted_loss.sum() / valid_label_mask.sum()\n",
    "        weighted_loss = weighted_loss.sum() / valid_label_mask.sum()\n",
    "\n",
    "        return weighted_loss, Non_weighted_loss, pos_loss, neg_loss\n",
    "    \n",
    "    def MaskedLM_loss(self, batch_labels, batch_predictions):\n",
    "\n",
    "        loss_fn = CrossEntropyLoss(ignore_index=-1)\n",
    "        vocab_size = self.hparams.vocab_size\n",
    "        loss = loss_fn(batch_predictions.view(-1, vocab_size), \n",
    "                batch_labels.view(-1))  \n",
    "        return loss  \n",
    "    \n",
    "    def Physchem_loss(self, batch_labels, batch_predictions):\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(batch_predictions, batch_labels)\n",
    "        return loss  \n",
    "\n",
    "    def unpack_batch(self, batch):\n",
    "        batch, valid = batch\n",
    "        (corrupted_batch_inputs,clean_batch_inputs),  corrupted_batch_labels = batch\n",
    "        return corrupted_batch_inputs,clean_batch_inputs,corrupted_batch_labels\n",
    "\n",
    "    def combine_invitro_invivo_batch(self, invitro_batch, invivo_batch):\n",
    "        device = self.hparams.device\n",
    "        # unpack invivo, invitro batch\n",
    "        invitro_corrupted_inputs, invitro_clean_inputs, invitro_labels = self.unpack_batch(invitro_batch)\n",
    "        invivo_corrupted_inputs, invivo_clean_inputs, invivo_labels = self.unpack_batch(invivo_batch)\n",
    "\n",
    "        # combine inputs\n",
    "        corrupted_inputs = {k: torch.cat([invitro_corrupted_inputs[k], invivo_corrupted_inputs[k]], dim=0).to(device) for k in invitro_corrupted_inputs.keys()}\n",
    "        clean_inputs = {k: torch.cat([invitro_clean_inputs[k], invivo_clean_inputs[k]], dim=0).to(device) for k in invitro_clean_inputs.keys()}\n",
    "\n",
    "        # missing labels\n",
    "        missing_invitro_labels = torch.full((invivo_labels[\"invitro\"].shape[0], 1234), -1, device = device)\n",
    "        missing_invivo_labels = torch.full((invitro_labels[\"invitro\"].shape[0], 50), -1, device = device)\n",
    "\n",
    "        # combine labels\n",
    "        batch_labels = {\"lm_label_ids\":torch.cat([invitro_labels[\"lm_label_ids\"], invivo_labels[\"lm_label_ids\"]], dim=0).to(device),\n",
    "                        \"unmasked_lm_label_ids\":torch.cat([invitro_labels[\"unmasked_lm_label_ids\"], invivo_labels[\"unmasked_lm_label_ids\"]], dim=0).to(device),\n",
    "                        \"physchem_props\":torch.cat([invitro_labels[\"physchem_props\"], invivo_labels[\"physchem_props\"]], dim=0).to(device),\n",
    "                        \"invitro\":torch.cat([invitro_labels[\"invitro\"].squeeze(), missing_invitro_labels], dim=0).to(device),\n",
    "                        \"invivo\":torch.cat([invivo_labels[\"invitro\"].squeeze(), missing_invivo_labels], dim=0).to(device),\n",
    "                        }\n",
    "        return corrupted_inputs, clean_inputs, batch_labels\n",
    "    \n",
    "    def step(self, batch, optimizer_idx):\n",
    "        ######## get batch ##########################\n",
    "        invitro_batch, invivo_batch = batch\n",
    "        corrupted_batch_inputs,clean_batch_inputs, corrupted_batch_labels = self.combine_invitro_invivo_batch(invitro_batch, invivo_batch)\n",
    "        \n",
    "        if optimizer_idx == 0:\n",
    "            Masked_token_pred, _,_,_ = self.forward(corrupted_batch_inputs)\n",
    "            embedding_loss = self.MaskedLM_loss(corrupted_batch_labels[\"lm_label_ids\"], Masked_token_pred)\n",
    "            \n",
    "            if self.hparams.compute_classification == True:\n",
    "            # save predictions for accuracy calculations\n",
    "                self.training_step_Masked_token_labels.append(corrupted_batch_labels[\"lm_label_ids\"].long().detach().cpu())\n",
    "                self.training_step_Masked_token_pred.append(Masked_token_pred.detach().cpu())\n",
    "\n",
    "            return {\"loss\": embedding_loss,\n",
    "                    \"embedding_loss\": embedding_loss,\n",
    "                    \"masking_loss\": embedding_loss}\n",
    "        \n",
    "        if optimizer_idx == 1:\n",
    "            _, Physchem_pred, _, _ = self.forward(clean_batch_inputs)\n",
    "            physchem_loss = self.Physchem_loss(corrupted_batch_labels[\"physchem_props\"], Physchem_pred)\n",
    "\n",
    "            if self.hparams.compute_classification == True:\n",
    "            # save predictions for accuracy calculations\n",
    "                self.training_step_physchem_labels.append(corrupted_batch_labels[\"physchem_props\"].long().detach().cpu())\n",
    "                self.training_step_physchem_pred.append(Physchem_pred.detach().cpu())\n",
    "  \n",
    "            return {\"loss\": physchem_loss,\n",
    "                    \"physchem_loss\": physchem_loss}\n",
    "        \n",
    "        if optimizer_idx == 2:\n",
    "            _, _, invitro_pred, _ = self.forward(clean_batch_inputs)\n",
    "            invitro_weighted_loss, invitro_Non_weighted_loss, invitro_pos_loss, invitro_neg_loss = self._compute_loss(corrupted_batch_labels[\"invitro\"], invitro_pred, targets=\"invitro\")\n",
    "            \n",
    "            if self.hparams.compute_classification == True:\n",
    "            # save predictions for accuracy calculations\n",
    "                self.training_step_invitro_labels.append(corrupted_batch_labels[\"invitro\"].long().detach().cpu())\n",
    "                self.training_step_invitro_pred.append(torch.sigmoid(invitro_pred.detach().cpu()))\n",
    "\n",
    "            return {\"loss\": invitro_weighted_loss,\n",
    "                    \"invitro_weighted_loss\": invitro_weighted_loss,\n",
    "                    \"invitro_Non_weighted_loss\": invitro_Non_weighted_loss,\n",
    "                    \"invitro_pos_loss\": invitro_pos_loss,\n",
    "                    \"invitro_neg_loss\": invitro_neg_loss,}\n",
    "        \n",
    "        if optimizer_idx == 3:\n",
    "            _, _, _, invivo_pred = self.forward(clean_batch_inputs)\n",
    "            invivo_weighted_loss, invivo_Non_weighted_loss, invivo_pos_loss, invivo_neg_loss = self._compute_loss(corrupted_batch_labels[\"invivo\"], invivo_pred, targets=\"invivo\")\n",
    "            \n",
    "            if self.hparams.compute_classification == True:\n",
    "                # save predictions for accuracy calculations\n",
    "                self.training_step_invivo_labels.append(corrupted_batch_labels[\"invivo\"].long().detach().cpu())\n",
    "                self.training_step_invivo_pred.append(torch.sigmoid(invivo_pred.detach().cpu()))\n",
    "\n",
    "            return {\"loss\": invivo_weighted_loss,\n",
    "                    \"invivo_weighted_loss\": invivo_weighted_loss,\n",
    "                    \"invivo_Non_weighted_loss\": invivo_Non_weighted_loss,\n",
    "                    \"invivo_pos_loss\": invivo_pos_loss,\n",
    "                    \"invivo_neg_loss\": invivo_neg_loss}\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        return self.step(batch, optimizer_idx)\n",
    "        \n",
    "    \n",
    "    def training_step_end(self, outputs):\n",
    "        # Define the step prefix\n",
    "        # Calculate mean losses\n",
    "        losses = {key: outputs[key].mean() for key in outputs.keys()}\n",
    "        \n",
    "        # Log the losses with WandB\n",
    "        log_dict = {f'train_{key}_step': value.item() for key, value in losses.items()}\n",
    "        log_dict[\"global_step\"] = self.trainer.global_step\n",
    "        wandb.log(log_dict)\n",
    "        \n",
    "        '''\n",
    "        ################################################################\n",
    "        # save checkpoint in between\n",
    "        ################################################################\n",
    "        interval_batches = int(0.1 * self.hparams.num_batches)\n",
    "        epoch, global_step = self.trainer.current_epoch, self.trainer.global_step + 1\n",
    "        if (global_step % interval_batches == 0) and (epoch == 0):\n",
    "            # Log the current epoch and step for clarity\n",
    "            print(f\"Saving checkpoint at epoch {epoch}, step {global_step}\")\n",
    "            filename = f\"epoch_{epoch}_step_{global_step}.ckpt\"\n",
    "            ckpt_path = os.path.join(self.trainer.checkpoint_callback.dirpath, filename)\n",
    "            self.trainer.save_checkpoint(ckpt_path)\n",
    "        '''\n",
    "        return losses\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ######## get batch ##########################\n",
    "            invitro_batch, invivo_batch = batch\n",
    "            corrupted_batch_inputs,clean_batch_inputs, corrupted_batch_labels = self.combine_invitro_invivo_batch(invitro_batch, invivo_batch)\n",
    "            \n",
    "            ######## forward pass, and losses ##########\n",
    "            Masked_token_pred, _,_,_ = self.forward(corrupted_batch_inputs)\n",
    "            _, Physchem_pred, invitro_pred, invivo_pred = self.forward(clean_batch_inputs)\n",
    "\n",
    "            embedding_loss = self.MaskedLM_loss(corrupted_batch_labels[\"lm_label_ids\"], Masked_token_pred)\n",
    "            physchem_loss = self.Physchem_loss(corrupted_batch_labels[\"physchem_props\"], Physchem_pred)\n",
    "            invitro_weighted_loss, invitro_Non_weighted_loss, invitro_pos_loss, invitro_neg_loss = self._compute_loss(corrupted_batch_labels[\"invitro\"], invitro_pred, targets=\"invitro\")\n",
    "            invivo_weighted_loss, invivo_Non_weighted_loss, invivo_pos_loss, invivo_neg_loss = self._compute_loss(corrupted_batch_labels[\"invivo\"], invivo_pred, targets=\"invivo\")\n",
    "            \n",
    "            if self.hparams.compute_classification == True:\n",
    "                # save predictions for accuracy calculations\n",
    "\n",
    "                self.val_step_Masked_token_labels.append(corrupted_batch_labels[\"lm_label_ids\"].long().detach().cpu())\n",
    "                self.val_step_Masked_token_pred.append(Masked_token_pred.detach().cpu())\n",
    "\n",
    "                self.val_step_invitro_labels.append(corrupted_batch_labels[\"invitro\"].long().detach().cpu())\n",
    "                self.val_step_invitro_pred.append(torch.sigmoid(invitro_pred.detach().cpu()))\n",
    "\n",
    "                self.val_step_invivo_labels.append(corrupted_batch_labels[\"invivo\"].long().detach().cpu())\n",
    "                self.val_step_invivo_pred.append(torch.sigmoid(invivo_pred.detach().cpu()))\n",
    "\n",
    "                self.val_step_physchem_labels.append(corrupted_batch_labels[\"physchem_props\"].long().detach().cpu())\n",
    "                self.val_step_physchem_pred.append(Physchem_pred.detach().cpu())\n",
    "\n",
    "            return {\n",
    "                \"loss\": embedding_loss,\n",
    "                \"embedding_loss\": embedding_loss,\n",
    "                \"masking_loss\": embedding_loss,\n",
    "                \"physchem_loss\": physchem_loss,\n",
    "                \"invitro_weighted_loss\": invitro_weighted_loss,\n",
    "                \"invitro_Non_weighted_loss\": invitro_Non_weighted_loss,\n",
    "                \"invitro_pos_loss\": invitro_pos_loss,\n",
    "                \"invitro_neg_loss\": invitro_neg_loss,\n",
    "                \"invivo_weighted_loss\": invivo_weighted_loss,\n",
    "                \"invivo_Non_weighted_loss\": invivo_Non_weighted_loss,\n",
    "                \"invivo_pos_loss\": invivo_pos_loss,\n",
    "                \"invivo_neg_loss\": invivo_neg_loss\n",
    "            }\n",
    "    \n",
    "    def validation_step_end(self, outputs):\n",
    "        # Define the step prefix\n",
    "        step_prefix = \"val_\"\n",
    "       # Calculate mean losses\n",
    "        losses = {key: outputs[key].mean() for key in outputs.keys()}\n",
    "        # Log the losses with WandB\n",
    "        log_dict = {f'{step_prefix}{key}_step': value.item() for key, value in losses.items()}\n",
    "        del log_dict[f'{step_prefix}loss_step']\n",
    "\n",
    "        log_dict[\"global_step\"] = self.trainer.global_step\n",
    "        if self.trainer.global_step > 0:\n",
    "            wandb.log(log_dict)\n",
    "        return losses\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        # Save at epoch 0\n",
    "        if self.current_epoch == 10:\n",
    "            print(f\"Saving checkpoint before first epoch/step\")\n",
    "            epoch, global_step = self.trainer.current_epoch, self.trainer.global_step\n",
    "            filename = f\"epoch_{epoch}_step_{global_step}.ckpt\"\n",
    "            ckpt_path = os.path.join(self.trainer.checkpoint_callback.dirpath, filename)\n",
    "            self.trainer.save_checkpoint(ckpt_path)\n",
    "\n",
    "        if self.current_epoch <= self.hparams.unfreeze_epoch:\n",
    "            self.freeze_network()\n",
    "            print(f\"freezing the network, trainable parameters = {self.count_parameters(self)}\")\n",
    "             \n",
    "        if self.current_epoch > self.hparams.unfreeze_epoch:\n",
    "            self.unfreeze_model()\n",
    "            print(f\"unfreezing the network, trainable parameters = {self.count_parameters(self)}\")\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        step_prefix = \"train_\"\n",
    "        epoch, step = self.trainer.current_epoch + 1, self.trainer.global_step\n",
    "\n",
    "        # Collect predictions and true labels for the complete training set\n",
    "        invitro_labels = torch.cat(self.training_step_invitro_labels, dim=0)\n",
    "        invitro_pred = torch.cat(self.training_step_invitro_pred, dim=0)\n",
    "        \n",
    "        invivo_labels = torch.cat(self.training_step_invivo_labels, dim=0)\n",
    "        invivo_pred = torch.cat(self.training_step_invivo_pred, dim=0)\n",
    "\n",
    "        physchem_labels = torch.cat(self.training_step_physchem_labels, dim=0)\n",
    "        physchem_pred = torch.cat(self.training_step_physchem_pred, dim=0)\n",
    "\n",
    "        masking_labels = torch.cat(self.training_step_Masked_token_labels, dim=0)\n",
    "        masking_pred = torch.cat(self.training_step_Masked_token_pred, dim=0)\n",
    "\n",
    "        print(\"######## training ##############\")\n",
    "        print(\"invitro\",invitro_labels.shape, invitro_pred.shape)\n",
    "        print(\"invivo\",invivo_labels.shape, invivo_pred.shape)\n",
    "        print(\"physchem\",physchem_labels.shape, physchem_pred.shape)\n",
    "        print(\"masking\",masking_labels.shape, masking_pred.shape)\n",
    "\n",
    "        # save predictions\n",
    "        if self.hparams.save_pred:\n",
    "            file_name = self.hparams.pred_dir + f\"epoch_{epoch}_step_{step}_train_\"\n",
    "            torch.save(invitro_labels, file_name + \"invitro_labels.pt\")\n",
    "            torch.save(invitro_pred, file_name + \"invitro_pred.pt\")\n",
    "            torch.save(invivo_labels, file_name + \"invivo_labels.pt\")\n",
    "            torch.save(invivo_pred, file_name + \"invivo_pred.pt\") \n",
    "            torch.save(masking_labels, file_name + \"masking_labels.pt\")\n",
    "            torch.save(masking_pred, file_name + \"masking_pred.pt\") \n",
    "\n",
    "        # compute losses from predictions\n",
    "        physchem_loss = self.Physchem_loss(physchem_labels, physchem_pred)\n",
    "        invitro_weighted_loss, invitro_Non_weighted_loss, invitro_pos_loss, invitro_neg_loss = self._compute_loss(invitro_labels.float(), invitro_pred, targets=\"invitro\")\n",
    "        invivo_weighted_loss, invivo_Non_weighted_loss, invivo_pos_loss, invivo_neg_loss = self._compute_loss(invivo_labels.float(), invivo_pred, targets=\"invivo\")\n",
    "        masking_loss = self.MaskedLM_loss(masking_labels, masking_pred)\n",
    "        embedding_loss = masking_loss.clone()\n",
    "\n",
    "        # Create a dictionary to store the losses\n",
    "        log_dict = {\n",
    "            f'{step_prefix}embedding_loss_epoch': embedding_loss.item(),\n",
    "            f'{step_prefix}masking_loss_epoch': masking_loss.item(),\n",
    "            f'{step_prefix}physchem_loss_epoch': physchem_loss.item(),\n",
    "            f'{step_prefix}invitro_weighted_loss_epoch': invitro_weighted_loss.item(),\n",
    "            f'{step_prefix}invitro_Non_weighted_loss_epoch': invitro_Non_weighted_loss.item(),\n",
    "            f'{step_prefix}invitro_pos_loss_epoch': invitro_pos_loss.item(),\n",
    "            f'{step_prefix}invitro_neg_loss_epoch': invitro_neg_loss.item(),\n",
    "            f'{step_prefix}invivo_weighted_loss_epoch': invivo_weighted_loss.item(),\n",
    "            f'{step_prefix}invivo_Non_weighted_loss_epoch': invivo_Non_weighted_loss.item(),\n",
    "            f'{step_prefix}invivo_pos_loss_epoch': invivo_pos_loss.item(),\n",
    "            f'{step_prefix}invivo_neg_loss_epoch': invivo_neg_loss.item(),\n",
    "        }\n",
    "\n",
    "        # add step and epoch information\n",
    "        log_dict.update({\n",
    "            \"current_epoch\": epoch,\n",
    "            \"global_step\": step\n",
    "        })\n",
    "\n",
    "        # Log the learning rate at the end of each epoch\n",
    "        lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        log_dict.update({'learning_rate': lr})\n",
    "\n",
    "\n",
    "        if self.hparams.compute_classification == True:\n",
    "            \n",
    "            invitro_score_list =  self.compute_metrics(invitro_labels, invitro_pred, targets_type = \"invitro\")\n",
    "            invivo_score_list =  self.compute_metrics(invivo_labels, invivo_pred, targets_type = \"invivo\")\n",
    "            physchem_score =  self.compute_regression_metrics(physchem_labels, physchem_pred)\n",
    "\n",
    "            metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision','ECE_score','ACE_score']\n",
    "                \n",
    "            for i, score in enumerate(invitro_score_list):\n",
    "                log_dict.update({f'train_invitro_{metric[i]}':score.item()})\n",
    "\n",
    "            for i, score in enumerate(invivo_score_list):\n",
    "                log_dict.update({f'train_invivo_{metric[i]}':score.item()})\n",
    "\n",
    "            log_dict.update({f'train_physchem_MAE':physchem_score.item()})        \n",
    "\n",
    "            # Clear the lists to free memory for the next epoch\n",
    "            self.training_step_invitro_labels.clear()\n",
    "            self.training_step_invitro_pred.clear()\n",
    "\n",
    "            self.training_step_invivo_labels.clear()\n",
    "            self.training_step_invivo_pred.clear()\n",
    "\n",
    "            self.training_step_physchem_labels.clear()\n",
    "            self.training_step_physchem_pred.clear()\n",
    "\n",
    "            self.training_step_Masked_token_labels.clear()\n",
    "            self.training_step_Masked_token_pred.clear()\n",
    "\n",
    "            del invitro_labels,invitro_pred, invivo_labels, invivo_pred, physchem_labels, physchem_pred, masking_labels, masking_pred\n",
    "\n",
    "        wandb.log(log_dict)\n",
    "        return log_dict\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        step_prefix = \"val_\"\n",
    "        epoch, step = self.trainer.current_epoch + 1, self.trainer.global_step + 1\n",
    "\n",
    "        # Collect predictions and true labels for the complete validation set\n",
    "        invitro_labels = torch.cat(self.val_step_invitro_labels, dim=0)\n",
    "        invitro_pred = torch.cat(self.val_step_invitro_pred, dim=0)\n",
    "\n",
    "        invivo_labels = torch.cat(self.val_step_invivo_labels, dim=0)\n",
    "        invivo_pred = torch.cat(self.val_step_invivo_pred, dim=0)\n",
    "\n",
    "        physchem_labels = torch.cat(self.val_step_physchem_labels, dim=0)\n",
    "        physchem_pred = torch.cat(self.val_step_physchem_pred, dim=0)\n",
    "\n",
    "        masking_labels = torch.cat(self.val_step_Masked_token_labels, dim=0)\n",
    "        masking_pred = torch.cat(self.val_step_Masked_token_pred, dim=0)\n",
    "\n",
    "        print(\"######## validation ##############\")\n",
    "        print(\"invitro\",invitro_labels.shape, invitro_pred.shape)\n",
    "        print(\"invivo\",invivo_labels.shape, invivo_pred.shape)\n",
    "        print(\"physchem\",physchem_labels.shape, physchem_pred.shape)\n",
    "        print(\"masking\",masking_labels.shape, masking_pred.shape)\n",
    "\n",
    "        # Save predictions\n",
    "        if self.hparams.save_pred:\n",
    "            file_name = self.hparams.pred_dir + f\"epoch_{epoch}_step_{step}_val_\"\n",
    "            torch.save(invitro_labels, file_name + \"invitro_labels.pt\")\n",
    "            torch.save(invitro_pred, file_name + \"invitro_pred.pt\")\n",
    "            torch.save(invivo_labels, file_name + \"invivo_labels.pt\")\n",
    "            torch.save(invivo_pred, file_name + \"invivo_pred.pt\")\n",
    "            torch.save(masking_labels, file_name + \"masking_labels.pt\")\n",
    "            torch.save(masking_pred, file_name + \"masking_pred.pt\")\n",
    "\n",
    "        # Compute losses from predictions\n",
    "        physchem_loss = self.Physchem_loss(physchem_labels, physchem_pred)\n",
    "        invitro_weighted_loss, invitro_Non_weighted_loss, invitro_pos_loss, invitro_neg_loss = self._compute_loss(invitro_labels.float(), invitro_pred, targets=\"invitro\")\n",
    "        invivo_weighted_loss, invivo_Non_weighted_loss, invivo_pos_loss, invivo_neg_loss = self._compute_loss(invivo_labels.float(), invivo_pred, targets=\"invivo\")\n",
    "        masking_loss = self.MaskedLM_loss(masking_labels, masking_pred)\n",
    "        embedding_loss = masking_loss.clone()\n",
    "\n",
    "        # Create a dictionary to store the losses\n",
    "        log_dict = {\n",
    "            f'{step_prefix}embedding_loss_epoch': embedding_loss.item(),\n",
    "            f'{step_prefix}masking_loss_epoch': masking_loss.item(),\n",
    "            f'{step_prefix}physchem_loss_epoch': physchem_loss.item(),\n",
    "            f'{step_prefix}invitro_weighted_loss_epoch': invitro_weighted_loss.item(),\n",
    "            f'{step_prefix}invitro_Non_weighted_loss_epoch': invitro_Non_weighted_loss.item(),\n",
    "            f'{step_prefix}invitro_pos_loss_epoch': invitro_pos_loss.item(),\n",
    "            f'{step_prefix}invitro_neg_loss_epoch': invitro_neg_loss.item(),\n",
    "            f'{step_prefix}invivo_weighted_loss_epoch': invivo_weighted_loss.item(),\n",
    "            f'{step_prefix}invivo_Non_weighted_loss_epoch': invivo_Non_weighted_loss.item(),\n",
    "            f'{step_prefix}invivo_pos_loss_epoch': invivo_pos_loss.item(),\n",
    "            f'{step_prefix}invivo_neg_loss_epoch': invivo_neg_loss.item(),\n",
    "        }\n",
    "\n",
    "        # Add step and epoch information\n",
    "        log_dict.update({\n",
    "            \"current_epoch\": epoch,\n",
    "            \"global_step\": step\n",
    "        })\n",
    "\n",
    "        # Log the learning rate at the end of each epoch\n",
    "        lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        log_dict.update({'learning_rate': lr})\n",
    "\n",
    "        if self.hparams.compute_classification == True:\n",
    "            \n",
    "            invitro_score_list = self.compute_metrics(invitro_labels, invitro_pred, targets_type=\"invitro\")\n",
    "            invivo_score_list = self.compute_metrics(invivo_labels, invivo_pred, targets_type=\"invivo\")\n",
    "            physchem_score = self.compute_regression_metrics(physchem_labels, physchem_pred)\n",
    "\n",
    "            metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision', 'ECE_score', 'ACE_score']\n",
    "                \n",
    "            for i, score in enumerate(invitro_score_list):\n",
    "                log_dict.update({f'val_invitro_{metric[i]}': score.item()})\n",
    "\n",
    "            for i, score in enumerate(invivo_score_list):\n",
    "                log_dict.update({f'val_invivo_{metric[i]}': score.item()})\n",
    "\n",
    "            log_dict.update({f'val_physchem_MAE': physchem_score.item()})\n",
    "        \n",
    "            # Clear the lists to free memory for the next epoch\n",
    "            self.val_step_invitro_labels.clear()\n",
    "            self.val_step_invitro_pred.clear()\n",
    "\n",
    "            self.val_step_invivo_labels.clear()\n",
    "            self.val_step_invivo_pred.clear()\n",
    "\n",
    "            self.val_step_physchem_labels.clear()\n",
    "            self.val_step_physchem_pred.clear()\n",
    "\n",
    "            self.val_step_Masked_token_labels.clear()\n",
    "            self.val_step_Masked_token_pred.clear()\n",
    "\n",
    "            del invitro_labels, invitro_pred, invivo_labels, invivo_pred, physchem_labels, physchem_pred, masking_labels, masking_pred\n",
    "\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "        # Log the current epoch and step for clarity\n",
    "        print(f\"Saving checkpoint at epoch {epoch}, step {step}\")\n",
    "        filename = f\"epoch_{epoch}_step_{step}.ckpt\"\n",
    "        ckpt_path = os.path.join(self.trainer.checkpoint_callback.dirpath, filename)\n",
    "        self.trainer.save_checkpoint(ckpt_path)\n",
    "        return log_dict\n",
    "\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        weight_norm = self.l2_regularization()\n",
    "        weight_norm = {'weight_norm': weight_norm}\n",
    "        wandb.log(weight_norm)\n",
    "\n",
    "       # Then clean the cache\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            with torch.cuda.device(f'cuda:{gpu_id}'):\n",
    "                torch.cuda.empty_cache()\n",
    "        # then collect the garbage\n",
    "        gc.collect()\n",
    "        print(\"!!!!!!!!! ALL CLEAR !!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "    def compute_metrics(self, y_true, y_pred, targets_type): \n",
    "        self.eval()\n",
    "\n",
    "        targets =  y_true.cpu().detach().tolist()\n",
    "        preds = y_pred.cpu().detach().tolist()\n",
    "\n",
    "        if targets_type == \"invitro\":\n",
    "            num_of_tasks = len(self.hparams.invitro_columns)\n",
    "        if targets_type == \"invivo\":\n",
    "            num_of_tasks = len(self.hparams.invivo_columns)\n",
    "\n",
    "        targets = np.array(targets, dtype=np.int8).reshape(-1,num_of_tasks)\n",
    "        preds = np.array(preds, dtype=np.float16).reshape(-1,num_of_tasks)\n",
    "\n",
    "        #if self.hparams.missing == 'nan':\n",
    "        #    mask = ~np.isnan(targets)\n",
    "        \n",
    "        mask = (targets != -1)\n",
    "\n",
    "        roc_score, blc_acc, sensitivity, specificity, AUPR, f1, average_precision = [],[],[],[],[],[],[]\n",
    "        ECE_score, ACE_score = [],[]\n",
    "\n",
    "        for i in range(num_of_tasks):\n",
    "            \n",
    "            try:\n",
    "                # get valid targets, and convert logits to prob\n",
    "                valid_targets = targets[:,i][mask[:,i]]\n",
    "                valid_preds = expit(preds[:,i][mask[:,i]])\n",
    "                ECE= compute_ece(valid_targets, valid_preds, n_bins=10, equal_intervals = True)\n",
    "                ACE = compute_ece(valid_targets, valid_preds, n_bins=10, equal_intervals = False)\n",
    "                ECE_score.append(ECE)\n",
    "                ACE_score.append(ACE)\n",
    "            except:\n",
    "                ECE_score.append(np.nan)\n",
    "                ACE_score.append(np.nan)\n",
    "\n",
    "            try:\n",
    "                # ROC_AUC\n",
    "                fpr, tpr, th = roc_curve(valid_targets, valid_preds)\n",
    "                roc_score.append(auc(fpr, tpr))\n",
    "\n",
    "                # Balanced accuracy\n",
    "                balanced_accuracy = (tpr + (1 - fpr)) / 2\n",
    "                blc_acc.append(np.max(balanced_accuracy))\n",
    "\n",
    "                # sensitivity, specificity\n",
    "                optimal_threshold_index = np.argmax(balanced_accuracy)\n",
    "                optimal_threshold = th[optimal_threshold_index]\n",
    "                sensitivity.append(tpr[optimal_threshold_index])\n",
    "                specificity.append(1 - fpr[optimal_threshold_index])\n",
    "\n",
    "                # AUPR, F1\n",
    "                precision, recall, thresholds = precision_recall_curve(valid_targets, valid_preds)\n",
    "                AUPR.append(auc(recall, precision))\n",
    "                f1_sc = f1_score(valid_targets, self.prob_to_labels(valid_preds, optimal_threshold))\n",
    "                f1.append(f1_sc)\n",
    "                average_precision.append(average_precision_score(valid_targets, valid_preds))\n",
    "                \n",
    "            except:\n",
    "                roc_score.append(np.nan)\n",
    "                AUPR.append(np.nan)\n",
    "                average_precision.append(np.nan)\n",
    "                #print('Performance metric is null')\n",
    "                \n",
    "        self.train()\n",
    "        return np.nanmean(roc_score), np.nanmean(blc_acc), np.nanmean(sensitivity), np.nanmean(specificity), np.nanmean(AUPR), np.nanmean(f1), np.nanmean(average_precision),np.nanmean(ECE_score),np.nanmean(ACE_score)\n",
    "\n",
    "    def compute_regression_metrics(self, y_true, y_pred): \n",
    "        self.eval()\n",
    "\n",
    "        targets =  y_true.cpu().detach().tolist()\n",
    "        preds = y_pred.cpu().detach().tolist()\n",
    "\n",
    "        targets = np.array(targets).reshape(-1,self.hparams.num_physchem_properties)\n",
    "        preds = np.array(preds).reshape(-1,self.hparams.num_physchem_properties)\n",
    "        MAE = mean_absolute_error(targets, preds)\n",
    "                \n",
    "        self.train()\n",
    "        return MAE\n",
    "   \n",
    "    def compute_ece(y_true, y_prob, n_bins=10, equal_intervals = True):\n",
    "        # Calculate bin boundaries\n",
    "        if equal_intervals == True: # ECE\n",
    "            bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "        else:                       # ACE\n",
    "            bin_boundaries = np.percentile(y_prob, np.linspace(0, 100, n_bins + 1))\n",
    "        \n",
    "        # Calculate bin indices\n",
    "        bin_indices = np.digitize(y_prob, bin_boundaries[1:-1])\n",
    "        \n",
    "        ece = 0\n",
    "        total_samples = len(y_true)\n",
    "        \n",
    "        # Calculate ECE\n",
    "        for bin_idx in range(n_bins):\n",
    "            # Filter samples within the bin\n",
    "            bin_mask = bin_indices == bin_idx\n",
    "            bin_samples = np.sum(bin_mask)\n",
    "            \n",
    "            if bin_samples > 0:\n",
    "                # Calculate accuracy and confidence for the bin\n",
    "                bin_accuracy = np.mean(y_true[bin_mask])\n",
    "                bin_confidence = np.mean(y_prob[bin_mask])\n",
    "            \n",
    "                # Update ECE\n",
    "                ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
    "        \n",
    "        return ece\n",
    "\n",
    "    def prob_to_labels(self, pred, threshold):\n",
    "\t    return (pred >= threshold).astype('int')\n",
    "\n",
    "    def unfreeze_model(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def count_parameters(self, model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    def freeze_network(self):\n",
    "        # List of tasks to freeze\n",
    "        #tasks_to_freeze = ['encoder', 'Masked_LM_task', 'Physchem_task', 'invitro_task']\n",
    "        tasks_to_freeze = ['encoder']\n",
    "\n",
    "        \n",
    "        # Iterate over the tasks and freeze their parameters\n",
    "        for task_name in tasks_to_freeze:\n",
    "            task = getattr(self, task_name)\n",
    "            for param in task.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Count the total number of trainable parameters\n",
    "        total_trainable_params = self.count_parameters(self)\n",
    "\n",
    "        # Count the number of trainable parameters\n",
    "        heads_params = 0\n",
    "        heads_params += self.count_parameters(self.Masked_LM_task)\n",
    "        heads_params += self.count_parameters(self.Physchem_task)\n",
    "        heads_params += self.count_parameters(self.invitro_task)\n",
    "        heads_params += self.count_parameters(self.invivo_task)\n",
    "\n",
    "        # Assert that they are equal\n",
    "        assert total_trainable_params == heads_params, (\n",
    "            f\"Total trainable parameters ({total_trainable_params}) do not match heads parameters ({heads_params})\"\n",
    "        )\n",
    "\n",
    "    def compute_interval_wise_means(self, outputs, interval_size):\n",
    "        num_intervals = int(len(outputs) // interval_size)\n",
    "        interval_wise_means = []\n",
    "\n",
    "        for i in range(num_intervals):\n",
    "            start_idx = i * interval_size\n",
    "            end_idx = (i + 1) * interval_size\n",
    "\n",
    "            interval_outputs = outputs[start_idx:end_idx]\n",
    "\n",
    "            # Initialize a dictionary to accumulate sums\n",
    "            interval_sums = {key: torch.tensor(0.0) for key in interval_outputs[0].keys()}\n",
    "\n",
    "            # Sum the values for each key in the interval\n",
    "            for output in interval_outputs:\n",
    "                for key, value in output.items():\n",
    "                    interval_sums[key] += value\n",
    "\n",
    "            # Calculate the mean for each key\n",
    "            interval_means = {key: (value / interval_size).item() for key, value in interval_sums.items()}\n",
    "            \n",
    "            interval_wise_means.append(interval_means)\n",
    "\n",
    "        return interval_wise_means\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO: GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: SCHEDULER: cosine_annealing_warm_restarts num_batches=17740 num_training_steps=17740 warmup_steps=1774\n",
      "INFO: SCHEDULER: cosine_annealing_warm_restarts num_batches=17740 num_training_steps=17740 warmup_steps=1774\n",
      "INFO: SCHEDULER: cosine_annealing_warm_restarts num_batches=17740 num_training_steps=17740 warmup_steps=1774\n",
      "INFO: SCHEDULER: cosine_annealing_warm_restarts num_batches=17740 num_training_steps=17740 warmup_steps=1774\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ececc98460426fa1877594d6994dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112451044431914, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                       | Type                     | Params\n",
      "------------------------------------------------------------------------\n",
      "0 | non_weighted_creterian     | BCEWithLogitsLoss        | 0     \n",
      "1 | invitro_weighted_creterien | BCEWithLogitsLoss        | 0     \n",
      "2 | invitro_FL                 | FocalLoss                | 0     \n",
      "3 | invivo_weighted_creterien  | BCEWithLogitsLoss        | 0     \n",
      "4 | invivo_FL                  | FocalLoss                | 0     \n",
      "5 | encoder                    | SuperPositionalBertModel | 85 M  \n",
      "6 | Masked_LM_task             | MaskedLMTask             | 624 K \n",
      "7 | Physchem_task              | PhyschemTask             | 744 K \n",
      "8 | invitro_task               | InvitroTasks             | 1 M   \n",
      "9 | invivo_task                | InvivoTasks              | 105 K \n",
      "INFO: \n",
      "  | Name                       | Type                     | Params\n",
      "------------------------------------------------------------------------\n",
      "0 | non_weighted_creterian     | BCEWithLogitsLoss        | 0     \n",
      "1 | invitro_weighted_creterien | BCEWithLogitsLoss        | 0     \n",
      "2 | invitro_FL                 | FocalLoss                | 0     \n",
      "3 | invivo_weighted_creterien  | BCEWithLogitsLoss        | 0     \n",
      "4 | invivo_FL                  | FocalLoss                | 0     \n",
      "5 | encoder                    | SuperPositionalBertModel | 85 M  \n",
      "6 | Masked_LM_task             | MaskedLMTask             | 624 K \n",
      "7 | Physchem_task              | PhyschemTask             | 744 K \n",
      "8 | invitro_task               | InvitroTasks             | 1 M   \n",
      "9 | invivo_task                | InvivoTasks              | 105 K \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87d35b6ec8642dd9f41686769d071dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## validation ##############\n",
      "invitro torch.Size([800, 1234]) torch.Size([800, 1234])\n",
      "invivo torch.Size([800, 50]) torch.Size([800, 50])\n",
      "physchem torch.Size([800, 200]) torch.Size([800, 200])\n",
      "masking torch.Size([800, 128]) torch.Size([800, 128, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at epoch 1, step 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ed30412de040e681b75a328bc9b54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing the network, trainable parameters = 3013494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796de7702e03461a93e82af34ebcb441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## validation ##############\n",
      "invitro torch.Size([800, 1234]) torch.Size([800, 1234])\n",
      "invivo torch.Size([800, 50]) torch.Size([800, 50])\n",
      "physchem torch.Size([800, 200]) torch.Size([800, 200])\n",
      "masking torch.Size([800, 128]) torch.Size([800, 128, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at epoch 1, step 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: saving model to /projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k_ADME/BERT_pretraining_embeddings_model/encoder_masking_plus_physchem_invitro_invivo_heads/epoch=0-step=0_v0.ckpt\n",
      "INFO: \n",
      "Epoch 00000: saving model to /projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k_ADME/BERT_pretraining_embeddings_model/encoder_masking_plus_physchem_invitro_invivo_heads/epoch=0-step=0_v0.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc66203e92a4dd2a6684623bfb7cc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## validation ##############\n",
      "invitro torch.Size([800, 1234]) torch.Size([800, 1234])\n",
      "invivo torch.Size([800, 50]) torch.Size([800, 50])\n",
      "physchem torch.Size([800, 200]) torch.Size([800, 200])\n",
      "masking torch.Size([800, 128]) torch.Size([800, 128, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at epoch 1, step 10\n",
      "######## training ##############\n",
      "invitro torch.Size([800, 1234]) torch.Size([800, 1234])\n",
      "invivo torch.Size([800, 50]) torch.Size([800, 50])\n",
      "physchem torch.Size([800, 200]) torch.Size([800, 200])\n",
      "masking torch.Size([800, 128]) torch.Size([800, 128, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!! ALL CLEAR !!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "def wandb_init_model(model, \n",
    "                     config, \n",
    "                     train_dataloader,\n",
    "                     val_dataloader, \n",
    "                     model_type):\n",
    "    \n",
    "    default_root_dir = config[\"model_weights_dir\"]\n",
    "    max_epochs = config[\"max_epochs\"]\n",
    "    return_trainer = config[\"return_trainer\"]\n",
    "\n",
    "    wandb.init(\n",
    "            project= config[\"project_name\"],\n",
    "            dir = '/projects/home/mmasood1/Model_weights',\n",
    "            entity=\"arslan_masood\", \n",
    "            reinit = True, \n",
    "            config = config,\n",
    "            name = config[\"model_name\"],\n",
    "            settings=wandb.Settings(start_method=\"fork\"))\n",
    "    \n",
    "    # logger\n",
    "    model = model(config)\n",
    "    wandb_logger = WandbLogger( \n",
    "                        name = config[\"model_name\"],\n",
    "                        save_dir = '/projects/home/mmasood1/Model_weights',\n",
    "                        project= config[\"project_name\"],\n",
    "                        entity=\"arslan_masood\", \n",
    "                        log_model=False,\n",
    "                        )\n",
    "    \n",
    "    # checkpoint callback\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                                        filepath= default_root_dir + '{epoch}-{step}',\n",
    "                                        save_top_k=-1,\n",
    "                                        verbose = True)\n",
    "\n",
    "    # trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs= int(max_epochs),\n",
    "        distributed_backend= config[\"distributed_backend\"],\n",
    "        gpus = config[\"gpu\"],\n",
    "        logger = wandb_logger,\n",
    "        precision = config_dict[\"precision\"],\n",
    "        default_root_dir=default_root_dir,\n",
    "        checkpoint_callback=checkpoint_callback,\n",
    "        resume_from_checkpoint = config[\"pretrained_crash_model\"],\n",
    "        num_sanity_val_steps = 10,\n",
    "        #val_check_interval = 0.1, \n",
    "        #fast_dev_run = True,\n",
    "        limit_train_batches = int(10),\n",
    "        limit_val_batches = int(10),\n",
    "        val_check_interval = 0.5\n",
    "\n",
    "        )\n",
    "\n",
    "    # model fitting \n",
    "    trainer.fit(model, \n",
    "                train_dataloader = train_dataloader,\n",
    "                val_dataloaders = val_dataloader,\n",
    "                )\n",
    "    if return_trainer:\n",
    "        return model, trainer\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "#config_dict[\"model_name\"] = rf's{config_dict[\"seed\"]}_alpha_{config_dict[\"alpha\"]}_gamma_{config_dict[\"gamma\"]}_{config_dict[\"loss_type\"]}_{config_dict[\"l2_lambda\"]}_{config_dict[\"optim\"]}_Emb{config_dict[\"bert_output_dim\"]}_Task{config_dict[\"num_of_tasks\"]}_{config_dict[\"learning_rate_scheduler\"]}'\n",
    "seed_everything(config_dict[\"seed\"])\n",
    "trained_model, trainer = wandb_init_model(model = MolbertModel, \n",
    "                                                                train_dataloader = train_dataloader,\n",
    "                                                                val_dataloader =val_dataloader,\n",
    "                                                                config = config_dict, \n",
    "                                                                model_type = 'MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "config_dict = {}\n",
    "data_dir = '/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/SMILES_len_th_128/'\n",
    "\n",
    "config_dict['invitro_train'] = data_dir + \"train_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "config_dict['invitro_val'] = data_dir + \"test_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "config_dict['invitro_test'] = data_dir + \"test_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "\n",
    "\n",
    "############## invivo ###########################\n",
    "config_dict[\"invivo_train\"] = \"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/Data_for_BERT_finetuning/complete_training_set.csv\"\n",
    "config_dict[\"invivo_val\"] = \"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/Data_for_BERT_finetuning/complete_test_set.csv\"\n",
    "\n",
    "invitro_train = pd.read_pickle(config_dict['invitro_train'])\n",
    "invitro_train = invitro_train.sample(n = 2000, random_state = 0).reset_index(drop = True)\n",
    "invitro_train.to_pickle(data_dir + \"sampled_data/\" + \"invitro_train.pkl\")\n",
    "\n",
    "invitro_val = pd.read_pickle(config_dict['invitro_val'])\n",
    "invitro_val = invitro_val.sample(n = 1000, random_state = 0).reset_index(drop = True)\n",
    "invitro_val.to_pickle(data_dir + \"sampled_data/\" + \"invitro_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss without chunking: 1.996942162513733\n",
      "Loss with chunking: 1.9969419240951538\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dummy data\n",
    "labels = torch.randn(100000, 10)  # 100,000 samples, each with 10 features\n",
    "preds = torch.randn(100000, 10)\n",
    "\n",
    "# Dummy loss function: Mean Squared Error\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Compute loss without chunking\n",
    "total_loss_no_chunks = loss_fn(labels, preds).item()\n",
    "\n",
    "# Function to compute loss with chunking\n",
    "def compute_losses_in_chunks(loss_fn, labels, preds, chunk_size=10000):\n",
    "    total_loss = 0.0\n",
    "    for start in range(0, labels.size(0), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk_loss = loss_fn(labels[start:end], preds[start:end]) * (end - start)\n",
    "        total_loss += chunk_loss\n",
    "    total_loss /= labels.size(0)\n",
    "    return total_loss.item()\n",
    "\n",
    "# Compute loss with chunking\n",
    "total_loss_chunks = compute_losses_in_chunks(loss_fn, labels, preds)\n",
    "\n",
    "# Print results\n",
    "print(f\"Loss without chunking: {total_loss_no_chunks}\")\n",
    "print(f\"Loss with chunking: {total_loss_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multiple_losses_in_chunks(loss_fn, labels, preds, chunk_size=10000):\n",
    "    total_weighted_loss = 0.0\n",
    "    total_non_weighted_loss = 0.0\n",
    "    total_pos_loss = 0.0\n",
    "    total_neg_loss = 0.0\n",
    "    \n",
    "    for start in range(0, labels.size(0), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk_weighted_loss, chunk_non_weighted_loss, chunk_pos_loss, chunk_neg_loss = loss_fn(\n",
    "            labels[start:end], preds[start:end]\n",
    "        )\n",
    "        # Accumulate each loss component\n",
    "        total_weighted_loss += chunk_weighted_loss * (end - start)\n",
    "        total_non_weighted_loss += chunk_non_weighted_loss * (end - start)\n",
    "        total_pos_loss += chunk_pos_loss * (end - start)\n",
    "        total_neg_loss += chunk_neg_loss * (end - start)\n",
    "\n",
    "    # Normalize by the total number of samples\n",
    "    total_weighted_loss /= labels.size(0)\n",
    "    total_non_weighted_loss /= labels.size(0)\n",
    "    total_pos_loss /= labels.size(0)\n",
    "    total_neg_loss /= labels.size(0)\n",
    "    \n",
    "    return (\n",
    "        total_weighted_loss.item(),\n",
    "        total_non_weighted_loss.item(),\n",
    "        total_pos_loss.item(),\n",
    "        total_neg_loss.item(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5179/3943433709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming _compute_loss is your function that returns the four losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m invitro_weighted_loss, invitro_Non_weighted_loss, invitro_pos_loss, invitro_neg_loss = compute_multiple_losses_in_chunks(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvitro_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvitro_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming _compute_loss is your function that returns the four losses\n",
    "invitro_weighted_loss, invitro_Non_weighted_loss, invitro_pos_loss, invitro_neg_loss = compute_multiple_losses_in_chunks(\n",
    "    self._compute_loss, invitro_labels.float(), invitro_pred, chunk_size=10000\n",
    ")\n",
    "\n",
    "invivo_weighted_loss, invivo_Non_weighted_loss, invivo_pos_loss, invivo_neg_loss = compute_multiple_losses_in_chunks(\n",
    "    self._compute_loss, invivo_labels.float(), invivo_pred, chunk_size=10000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without chunking\n",
    "invitro_weighted_loss_no_chunks, invitro_Non_weighted_loss_no_chunks, invitro_pos_loss_no_chunks, invitro_neg_loss_no_chunks = self._compute_loss(invitro_labels.float(), invitro_pred)\n",
    "\n",
    "# With chunking\n",
    "invitro_weighted_loss_chunks, invitro_Non_weighted_loss_chunks, invitro_pos_loss_chunks, invitro_neg_loss_chunks = compute_multiple_losses_in_chunks(\n",
    "    self._compute_loss, invitro_labels.float(), invitro_pred, chunk_size=10000\n",
    ")\n",
    "\n",
    "# Print or assert to check for equality\n",
    "print(f\"Without chunking: {invitro_weighted_loss_no_chunks}, {invitro_Non_weighted_loss_no_chunks}, {invitro_pos_loss_no_chunks}, {invitro_neg_loss_no_chunks}\")\n",
    "print(f\"With chunking: {invitro_weighted_loss_chunks}, {invitro_Non_weighted_loss_chunks}, {invitro_pos_loss_chunks}, {invitro_neg_loss_chunks}\")\n",
    "\n",
    "# You can similarly do this for invivo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
